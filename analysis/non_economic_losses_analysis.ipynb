{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Non-Economic Losses Analysis - Setup\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.api.types import CategoricalDtype\n",
        "\n",
        "# Plotting style\n",
        "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (10, 6),\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"legend.frameon\": False,\n",
        "})\n",
        "\n",
        "# Constants\n",
        "INPUT_CSV = \"/Users/zhangruotian/Documents/apify_scraper/analysis/merged_all_flood_data.csv\"\n",
        "LOSS = [\n",
        "    \"displacement\",\"education_disruption\",\"health_trauma\",\"social_ties_loss\",\n",
        "    \"cultural_ritual_disruption\",\"caregiving_burden\",\"water_food_insecurity\",\n",
        "    \"infrastructure_access\",\"psychosocial_distress\"\n",
        "]\n",
        "PRESENT = [f\"loss_{k}_present\" for k in LOSS]\n",
        "CONF = [f\"loss_{k}_confidence\" for k in LOSS]\n",
        "\n",
        "# Depth and crowd categorical orders\n",
        "DEPTH_ORDER = [\"none\",\"ankle\",\"knee\",\"waist\",\"vehicle_height\",\"indoor_flood\",\"unknown\"]\n",
        "CROWD_ORDER = [\"1\",\"2-5\",\"6-20\",\">20\",\"unknown\"]\n",
        "\n",
        "print(\"✅ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data and unify timestamps\n",
        "RAW = pd.read_csv(INPUT_CSV)\n",
        "DF = RAW.copy()\n",
        "\n",
        "# Normalize platform label\n",
        "DF['platform'] = DF.get('source', '').astype(str).str.lower()\n",
        "\n",
        "# Unify timestamp column `ts` using platform-specific fields\n",
        "\n",
        "def parse_ts(row):\n",
        "    src = str(row.get('source', '')).lower()\n",
        "    if src == 'twitter':\n",
        "        return pd.to_datetime(row.get('created_at'), errors='coerce', utc=True)\n",
        "    return pd.to_datetime(row.get('uploaded_at_formatted'), errors='coerce', utc=True)\n",
        "\n",
        "DF['ts'] = DF.apply(parse_ts, axis=1)\n",
        "DF['date'] = DF['ts'].dt.date\n",
        "DF['week'] = DF['ts'].dt.to_period('W').dt.start_time\n",
        "\n",
        "# Basic normalization for key columns if missing or wrong dtype\n",
        "for col in PRESENT:\n",
        "    if col in DF.columns:\n",
        "        DF[col] = DF[col].astype('bool', errors='ignore')\n",
        "for col in CONF:\n",
        "    if col in DF.columns:\n",
        "        DF[col] = pd.to_numeric(DF[col], errors='coerce')\n",
        "\n",
        "# Ensure urgency_score exists\n",
        "if 'urgency_score' in DF.columns:\n",
        "    DF['urgency_score'] = pd.to_numeric(DF['urgency_score'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "print(f\"✅ Loaded rows: {len(DF)} | Valid timestamps: {(DF['ts'].notna()).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare columns: list-like parsing, categorical orders, derived fields\n",
        "\n",
        "# Parse list-like columns safely\n",
        "for col in ['damage_signs', 'context_area']:\n",
        "    if col in DF.columns:\n",
        "        DF[col] = DF[col].apply(\n",
        "            lambda x: x if isinstance(x, list)\n",
        "            else (json.loads(x) if isinstance(x, str) and x.strip().startswith('[') else ([] if pd.isna(x) else [str(x)]))\n",
        "        )\n",
        "    else:\n",
        "        DF[col] = [[] for _ in range(len(DF))]\n",
        "\n",
        "# Derived counts\n",
        "DF['damage_signs_count'] = DF['damage_signs'].apply(len)\n",
        "\n",
        "# Categorical orders for depth and crowd\n",
        "if 'water_depth_bin' in DF.columns:\n",
        "    DF['water_depth_bin'] = DF['water_depth_bin'].astype(CategoricalDtype(DEPTH_ORDER, ordered=True))\n",
        "else:\n",
        "    DF['water_depth_bin'] = pd.Categorical(['unknown'] * len(DF), categories=DEPTH_ORDER, ordered=True)\n",
        "\n",
        "if 'crowd_size_bin' in DF.columns:\n",
        "    DF['crowd_size_bin'] = DF['crowd_size_bin'].astype(CategoricalDtype(CROWD_ORDER, ordered=True))\n",
        "else:\n",
        "    DF['crowd_size_bin'] = pd.Categorical(['unknown'] * len(DF), categories=CROWD_ORDER, ordered=True)\n",
        "\n",
        "# Relief visible bool normalization\n",
        "if 'relief_visible' in DF.columns:\n",
        "    DF['relief_visible'] = DF['relief_visible'].astype('bool', errors='ignore')\n",
        "else:\n",
        "    DF['relief_visible'] = False\n",
        "\n",
        "# Region normalization placeholder (keeps as-is)\n",
        "if 'region' in DF.columns:\n",
        "    DF['region'] = DF['region'].astype(str)\n",
        "\n",
        "print(\"✅ Columns prepared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis 1: Frequency & Confidence\n",
        "\n",
        "# Prepare a tidy table of metrics per loss type\n",
        "rows = []\n",
        "for k in LOSS:\n",
        "    p = DF[f\"loss_{k}_present\"].mean(skipna=True) if f\"loss_{k}_present\" in DF.columns else np.nan\n",
        "    w = (DF[f\"loss_{k}_present\"] * DF[f\"loss_{k}_confidence\"]).mean(skipna=True) if f\"loss_{k}_confidence\" in DF.columns and f\"loss_{k}_present\" in DF.columns else np.nan\n",
        "    cbar = DF.loc[DF.get(f\"loss_{k}_present\", False) == True, f\"loss_{k}_confidence\"].mean(skipna=True) if f\"loss_{k}_confidence\" in DF.columns and f\"loss_{k}_present\" in DF.columns else np.nan\n",
        "    rows.append({\"loss\": k, \"occurrence_rate\": p, \"weighted_rate\": w, \"mean_conf_when_present\": cbar})\n",
        "\n",
        "freq_df = pd.DataFrame(rows).sort_values(\"weighted_rate\", ascending=False)\n",
        "freq_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Plot p_k vs ptilde_k\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bar_width = 0.4\n",
        "idx = np.arange(len(freq_df))\n",
        "ax.barh(idx + 0.2, freq_df['occurrence_rate'], height=bar_width, label='Occurrence rate (p_k)')\n",
        "ax.barh(idx - 0.2, freq_df['weighted_rate'], height=bar_width, label='Confidence-weighted (ptilde_k)')\n",
        "ax.set_yticks(idx)\n",
        "ax.set_yticklabels(freq_df['loss'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Rate')\n",
        "ax.set_title('Loss Type Frequency and Confidence-Weighted Frequency')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "freq_df.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis 2: Pairwise Co-occurrence (Jaccard)\n",
        "\n",
        "# Build Jaccard matrix over present flags\n",
        "loss_flags = DF[PRESENT].fillna(False).astype(bool)\n",
        "labels = [c.replace('loss_','').replace('_present','') for c in loss_flags.columns]\n",
        "\n",
        "jac = np.zeros((len(labels), len(labels)), dtype=float)\n",
        "for i in range(len(labels)):\n",
        "    a = loss_flags.iloc[:, i].to_numpy()\n",
        "    for j in range(len(labels)):\n",
        "        b = loss_flags.iloc[:, j].to_numpy()\n",
        "        inter = (a & b).sum()\n",
        "        union = (a | b).sum()\n",
        "        jac[i, j] = (inter / union) if union else 0.0\n",
        "\n",
        "jac_df = pd.DataFrame(jac, index=labels, columns=labels)\n",
        "\n",
        "# Plot upper-triangle heatmap\n",
        "mask = np.triu(np.ones_like(jac_df, dtype=bool), k=1)\n",
        "plt.figure(figsize=(9, 7))\n",
        "sns.heatmap(jac_df, mask=~mask, cmap='Blues', vmin=0, vmax=1, cbar_kws={'label':'Jaccard'})\n",
        "plt.title('Pairwise Co-occurrence (Jaccard) of Loss Types')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top co-occurring pairs\n",
        "pairs = []\n",
        "for i in range(len(labels)):\n",
        "    for j in range(i+1, len(labels)):\n",
        "        pairs.append((labels[i], labels[j], jac[i, j]))\n",
        "\n",
        "top_pairs = pd.DataFrame(pairs, columns=['loss_i','loss_j','jaccard']).sort_values('jaccard', ascending=False).head(15)\n",
        "top_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis 3: Relationship with Water Depth / Urgency\n",
        "\n",
        "# Convert depth bin to numeric level for trend computations\n",
        "DEPTH_TO_NUM = {v: i for i, v in enumerate(DEPTH_ORDER)}\n",
        "DF['depth_level_num'] = DF['water_depth_bin'].astype(str).map(DEPTH_TO_NUM).fillna(np.nan)\n",
        "\n",
        "# Dose-response: p_k by depth\n",
        "trend_rows = []\n",
        "for k in LOSS:\n",
        "    col = f\"loss_{k}_present\"\n",
        "    if col in DF.columns:\n",
        "        grp = DF.groupby('water_depth_bin')[col].mean()\n",
        "        for depth, val in grp.items():\n",
        "            trend_rows.append({'loss': k, 'water_depth_bin': depth, 'p_k': val})\n",
        "\n",
        "dose_df = pd.DataFrame(trend_rows)\n",
        "\n",
        "# Plot as small multiples\n",
        "g = sns.relplot(\n",
        "    data=dose_df, x='water_depth_bin', y='p_k', col='loss', col_wrap=3,\n",
        "    kind='line', marker='o', facet_kws={'sharey': False}\n",
        ")\n",
        "for ax in g.axes.flat:\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlabel('Water depth bin')\n",
        "    ax.set_ylabel('Occurrence rate')\n",
        "plt.suptitle('Dose-Response: Loss occurrence vs Water Depth', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Urgency vs depth and damage\n",
        "urg_df = DF[['depth_level_num','damage_signs_count','urgency_score']].copy()\n",
        "urg_corr = urg_df.corr(method='spearman')\n",
        "print(\"Spearman correlations among depth, damage, urgency:\\n\", urg_corr)\n",
        "\n",
        "# Partial visuals\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.boxplot(data=DF, x='water_depth_bin', y='urgency_score', ax=axes[0])\n",
        "axes[0].set_title('Urgency by Depth')\n",
        "axes[0].set_xlabel('Depth bin')\n",
        "axes[0].set_ylabel('Urgency (0-5)')\n",
        "\n",
        "sns.scatterplot(data=DF, x='damage_signs_count', y='urgency_score', alpha=0.4, ax=axes[1])\n",
        "axes[1].set_title('Urgency vs Damage Signs Count')\n",
        "axes[1].set_xlabel('Damage signs count')\n",
        "axes[1].set_ylabel('Urgency (0-5)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis 4: Temporal Change\n",
        "\n",
        "# Daily series for each loss: p_k(t)\n",
        "TS = DF.set_index('ts').sort_index()\n",
        "TS_daily = TS.resample('D')[PRESENT].mean()\n",
        "TS_weekly = TS.resample('W')[PRESENT].mean()\n",
        "\n",
        "# 7-day moving average (centered)\n",
        "TS_daily_ma7 = TS_daily.rolling(window=7, min_periods=1, center=True).mean()\n",
        "\n",
        "# Plot 9 facets for daily MA\n",
        "melt = TS_daily_ma7.reset_index().melt(id_vars='ts', var_name='loss_present', value_name='rate')\n",
        "melt['loss'] = melt['loss_present'].str.replace('loss_','').str.replace('_present','', regex=False)\n",
        "\n",
        "g = sns.relplot(\n",
        "    data=melt, x='ts', y='rate', col='loss', col_wrap=3,\n",
        "    kind='line', height=2.8, facet_kws={'sharey': False}\n",
        ")\n",
        "for ax in g.axes.flat:\n",
        "    ax.set_ylabel('Rate (7D MA)')\n",
        "    ax.set_xlabel('Date')\n",
        "plt.suptitle('Temporal Evolution of Loss Occurrence (7D MA)', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Lag check: cross-correlation (naive) between infra_access and education_disruption weekly\n",
        "from pandas import Series\n",
        "\n",
        "def xcorr(a: Series, b: Series, max_lag=8):\n",
        "    # Center and fill\n",
        "    a = a.fillna(0) - a.mean()\n",
        "    b = b.fillna(0) - b.mean()\n",
        "    lags = list(range(-max_lag, max_lag+1))\n",
        "    vals = []\n",
        "    for L in lags:\n",
        "        if L < 0:\n",
        "            vals.append(a.shift(-L).corr(b))\n",
        "        else:\n",
        "            vals.append(a.corr(b.shift(L)))\n",
        "    return pd.DataFrame({'lag': lags, 'xcorr': vals})\n",
        "\n",
        "ia = TS_weekly['loss_infrastructure_access_present']\n",
        "ed = TS_weekly['loss_education_disruption_present']\n",
        "xc = xcorr(ia, ed, max_lag=12)\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "plt.stem(xc['lag'], xc['xcorr'], use_line_collection=True)\n",
        "plt.axhline(0, color='k', lw=0.8)\n",
        "plt.title('Cross-correlation: infra_access vs education_disruption (weekly)')\n",
        "plt.xlabel('Lag (weeks); positive means infra leads')\n",
        "plt.ylabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "xc.sort_values('xcorr', ascending=False).head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Analyses: Visual Cues, Relief, Demographics, Sentiment, Recovery\n",
        "\n",
        "# 1) Crowd size × infrastructure_access\n",
        "if 'loss_infrastructure_access_present' in DF.columns:\n",
        "    infra_crowd = (\n",
        "        DF.groupby('crowd_size_bin')['loss_infrastructure_access_present']\n",
        "          .mean().reindex(CROWD_ORDER)\n",
        "    )\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.barplot(x=infra_crowd.index, y=infra_crowd.values, color='#4C72B0')\n",
        "    plt.ylim(0,1)\n",
        "    plt.title('Infrastructure Access Disruption by Crowd Size')\n",
        "    plt.xlabel('Crowd size bin')\n",
        "    plt.ylabel('Occurrence rate')\n",
        "    plt.show()\n",
        "\n",
        "# 2) Relief visibility by depth and context\n",
        "relief_by_depth = DF.groupby('water_depth_bin')['relief_visible'].mean()\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=relief_by_depth.index.astype(str), y=relief_by_depth.values, color='#55A868')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Relief Visible by Depth')\n",
        "plt.xlabel('Depth bin')\n",
        "plt.ylabel('Share with relief visible')\n",
        "plt.show()\n",
        "\n",
        "# Relief actor type distribution where relief is visible\n",
        "if 'relief_actor_type' in DF.columns:\n",
        "    actors = DF.loc[DF['relief_visible'] == True, 'relief_actor_type'].fillna('unknown').value_counts(normalize=True)\n",
        "    actors.plot(kind='bar', color='#C44E52', figsize=(6,4), title='Relief Actor Types (where relief visible)')\n",
        "    plt.ylabel('Share')\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "# 3) Damage sign frequencies\n",
        "from collections import Counter\n",
        "all_signs = Counter(s for row in DF['damage_signs'] for s in row if s and s != 'none')\n",
        "damage_freq = (pd.Series(all_signs).sort_values(ascending=False) if all_signs else pd.Series(dtype=float))\n",
        "damage_freq.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Composite Indicators: NELI and VISI\n",
        "\n",
        "# NELI: sum_k 1 * present_k * conf_k (uniform weights)\n",
        "conf_cols = [f'loss_{k}_confidence' for k in LOSS]\n",
        "pres_cols = [f'loss_{k}_present' for k in LOSS]\n",
        "\n",
        "# Defensive: fill missing with zeros/False\n",
        "conf_mat = DF[conf_cols].fillna(0.0)\n",
        "pres_mat = DF[pres_cols].fillna(False).astype(bool)\n",
        "NELI = (conf_mat.values * pres_mat.values).sum(axis=1)\n",
        "DF['NELI'] = NELI\n",
        "\n",
        "# VISI: depth_level_num + damage_signs_count + crowd_size_num + 1*relief_visible\n",
        "crowd_to_num = {v: i for i, v in enumerate(CROWD_ORDER)}\n",
        "DF['crowd_size_num'] = DF['crowd_size_bin'].astype(str).map(crowd_to_num)\n",
        "DF['VISI'] = DF['depth_level_num'].fillna(0) + DF['damage_signs_count'].fillna(0) + DF['crowd_size_num'].fillna(0) + DF['relief_visible'].astype(int)\n",
        "\n",
        "# Distributions\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "sns.histplot(DF['NELI'], bins=30, ax=axes[0])\n",
        "axes[0].set_title('NELI Distribution')\n",
        "\n",
        "sns.histplot(DF['VISI'], bins=30, ax=axes[1])\n",
        "axes[1].set_title('VISI Distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation and scatter by scene/context (high level)\n",
        "cols = ['NELI','VISI','urgency_score','depth_level_num','damage_signs_count']\n",
        "print(DF[cols].corr(method='spearman'))\n",
        "\n",
        "sns.scatterplot(data=DF, x='VISI', y='NELI', alpha=0.3)\n",
        "plt.title('NELI vs VISI')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality & Validation\n",
        "\n",
        "# Visual Verifiability Score per plan\n",
        "# +0.2 if water_depth_bin in set, +0.2 if damage_signs includes any of listed, +0.2 if any loss present with conf>=0.8,\n",
        "# +0.2 if scene_type is ground_outdoor/indoor, +0.2 if context_area != 'unknown'\n",
        "\n",
        "def compute_verifiability(row):\n",
        "    score = 0.0\n",
        "    # depth\n",
        "    if str(row.get('water_depth_bin','unknown')) in ['none','ankle','knee','waist','vehicle_height','indoor_flood']:\n",
        "        score += 0.2\n",
        "    # damage signs\n",
        "    ds = row.get('damage_signs', []) or []\n",
        "    allowed = {'road_blocked','house_inundated','bridge_damage','school_closed_sign','clinic_closed_sign','power_outage_sign'}\n",
        "    if any(s in allowed for s in ds):\n",
        "        score += 0.2\n",
        "    # loss present with conf>=0.8\n",
        "    ok = False\n",
        "    for k in LOSS:\n",
        "        if row.get(f'loss_{k}_present') and (row.get(f'loss_{k}_confidence', 0) >= 0.8):\n",
        "            ok = True; break\n",
        "    if ok:\n",
        "        score += 0.2\n",
        "    # scene type\n",
        "    scene_ok = False\n",
        "    for s in ['scene_ground_outdoor','scene_indoor']:\n",
        "        if s in row and bool(row[s]):\n",
        "            scene_ok = True\n",
        "    if scene_ok:\n",
        "        score += 0.2\n",
        "    # context area known\n",
        "    ctx = row.get('context_area', []) or []\n",
        "    if any(c != 'unknown' for c in ctx):\n",
        "        score += 0.2\n",
        "    return score\n",
        "\n",
        "DF['verifiability'] = DF.apply(compute_verifiability, axis=1)\n",
        "\n",
        "# Histogram and by platform\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "sns.histplot(DF['verifiability'], bins=[0,0.2,0.4,0.6,0.8,1.0], ax=axes[0])\n",
        "axes[0].set_title('Visual Verifiability Distribution')\n",
        "\n",
        "sns.boxplot(data=DF, x='platform', y='verifiability', ax=axes[1])\n",
        "axes[1].set_title('Verifiability by Platform')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Unknown proportions\n",
        "unknown_depth = (DF['water_depth_bin'].astype(str) == 'unknown').mean()\n",
        "unknown_area = DF['context_area'].apply(lambda xs: len(xs)==0 or all(x=='unknown' for x in xs)).mean()\n",
        "unknown_scene = (DF.get('scene_aerial', False) == False) & (DF.get('scene_ground_outdoor', False) == False) & (DF.get('scene_indoor', False) == False)\n",
        "unknown_scene = unknown_scene.mean() if hasattr(unknown_scene, 'mean') else np.nan\n",
        "\n",
        "print({\n",
        "    'unknown_water_depth_bin': unknown_depth,\n",
        "    'unknown_context_area': unknown_area,\n",
        "    'unknown_scene_type_all_false': unknown_scene\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified: Platform comparison (Twitter vs TikTok)\n",
        "\n",
        "platform_metrics = []\n",
        "for plat, sub in DF.groupby('platform'):\n",
        "    metrics = {'platform': plat, 'n': len(sub)}\n",
        "    for k in LOSS:\n",
        "        p = sub.get(f'loss_{k}_present', pd.Series(dtype=float)).mean()\n",
        "        w = (sub.get(f'loss_{k}_present', 0) * sub.get(f'loss_{k}_confidence', 0)).mean()\n",
        "        metrics[f'{k}_p'] = p\n",
        "        metrics[f'{k}_w'] = w\n",
        "    metrics['relief_visible_share'] = sub.get('relief_visible', pd.Series(dtype=float)).mean()\n",
        "    metrics['mean_urgency'] = sub.get('urgency_score', pd.Series(dtype=float)).mean()\n",
        "    platform_metrics.append(metrics)\n",
        "\n",
        "plat_df = pd.DataFrame(platform_metrics)\n",
        "plat_df\n",
        "\n",
        "# Visual: selected losses and relief\n",
        "sel = ['infrastructure_access','water_food_insecurity','education_disruption','psychosocial_distress']\n",
        "plot_df = (\n",
        "    plat_df.melt(id_vars=['platform','n'], value_vars=[f'{k}_p' for k in sel], var_name='metric', value_name='rate')\n",
        ")\n",
        "plot_df['loss'] = plot_df['metric'].str.replace('_p','', regex=False)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=plot_df, x='loss', y='rate', hue='platform')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Platform comparison: key loss occurrence rates')\n",
        "plt.ylabel('Rate')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=plat_df, x='platform', y='relief_visible_share')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Relief visibility by platform')\n",
        "plt.ylabel('Share')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified: Regional comparison\n",
        "\n",
        "if 'region' in DF.columns:\n",
        "    # Keep top-N by count\n",
        "    topN = 12\n",
        "    region_sizes = DF['region'].value_counts()\n",
        "    top_regions = set(region_sizes.head(topN).index)\n",
        "    REG = DF[DF['region'].isin(top_regions)].copy()\n",
        "\n",
        "    # Heatmap of key loss rates by region\n",
        "    key_losses = ['infrastructure_access','water_food_insecurity','education_disruption','psychosocial_distress']\n",
        "    mat = REG.groupby('region')[[f'loss_{k}_present' for k in key_losses]].mean().reindex(region_sizes.head(topN).index)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.heatmap(mat, annot=True, fmt='.2f', cmap='YlOrBr', vmin=0, vmax=1)\n",
        "    plt.title('Regional comparison: key loss occurrence rates (top regions by count)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Faceted time series by region for infrastructure access\n",
        "    reg_ts = (\n",
        "        REG.set_index('ts').sort_index()\n",
        "           .groupby('region')['loss_infrastructure_access_present']\n",
        "           .resample('W').mean().reset_index()\n",
        "    )\n",
        "    g = sns.relplot(\n",
        "        data=reg_ts, x='ts', y='loss_infrastructure_access_present', col='region', col_wrap=4,\n",
        "        kind='line', height=2.2, facet_kws={'sharey': False}\n",
        "    )\n",
        "    for ax in g.axes.flat:\n",
        "        ax.set_ylabel('Rate (weekly)')\n",
        "        ax.set_xlabel('')\n",
        "    plt.suptitle('Infrastructure Access over Time by Region (Weekly)', y=1.02)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Region column not found.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Findings (Draft)\n",
        "\n",
        "- Frequency & Confidence: Highlight top-3 by weighted rate; note low-confidence/rare classes.\n",
        "- Co-occurrence: Note strongest pairs (e.g., infra_access ↔ water_food_insecurity if observed).\n",
        "- Depth & Urgency: Urgency rises with depth/damage; list Spearman signs.\n",
        "- Temporal: Report lag with highest cross-corr between infra_access and education_disruption.\n",
        "- Additional: Relief appears more at deeper levels; actor type mix shown.\n",
        "- Composite: NELI correlates positively with VISI and urgency.\n",
        "- Platform: Contrast key loss rates and relief visibility.\n",
        "- Region: Surface regional heterogeneity; show top varying regions.\n",
        "\n",
        "Refine after reviewing actual plots/tables above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
