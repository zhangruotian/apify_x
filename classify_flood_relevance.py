#!/usr/bin/env python3
"""
ä½¿ç”¨ Ollama Qwen VLM æ¨¡å‹å¯¹ TikTok æ•°æ®è¿›è¡Œæ´ªæ°´ç›¸å…³æ€§æ ‡æ³¨
"""

import json
import os
import re
import base64
import requests
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import time
import asyncio
import aiohttp
from io import BytesIO
from PIL import Image


class OllamaVLMClassifier:
    """ä½¿ç”¨ Ollama VLM è¿›è¡Œæ´ªæ°´ç›¸å…³æ€§åˆ†ç±»"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:11434", model: str = "qwen3-vl:30b-a3b-instruct-q4_K_M"):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self._session = None  # ç”¨äºå¼‚æ­¥è¯·æ±‚çš„ session
        self._image_cache = {}  # LRU ç¼“å­˜ï¼šè·¯å¾„ -> base64
        self._cache_max_size = 1000  # ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
        
    def _compress_image(self, image_path: Path, max_short_side: int = 768, quality: int = 85) -> bytes:
        """å‹ç¼©å›¾ç‰‡ï¼šçŸ­è¾¹â‰¤768ï¼ŒJPEG quality=85ï¼Œoptimize=True"""
        try:
            with Image.open(image_path) as img:
                # è½¬æ¢ä¸º RGBï¼ˆå¦‚æœæ˜¯ RGBA æˆ–å…¶ä»–æ ¼å¼ï¼‰
                if img.mode in ('RGBA', 'LA', 'P'):
                    # åˆ›å»ºç™½è‰²èƒŒæ™¯
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'P':
                        img = img.convert('RGBA')
                    background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆçŸ­è¾¹â‰¤768ï¼‰
                width, height = img.size
                if min(width, height) > max_short_side:
                    ratio = max_short_side / min(width, height)
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)
                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                
                # JPEG å‹ç¼©
                output = BytesIO()
                img.save(output, format='JPEG', quality=quality, optimize=True)
                return output.getvalue()
        except Exception as e:
            # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå›¾
            print(f"âš ï¸  Image compression failed for {image_path}: {e}, using original")
            with open(image_path, "rb") as f:
                return f.read()
    
    def encode_image(self, image_path: str, use_cache: bool = True) -> str:
        """å°†å›¾åƒæ–‡ä»¶ç¼–ç ä¸º base64ï¼ˆå¸¦å‹ç¼©å’Œç¼“å­˜ï¼‰"""
        image_path_str = str(image_path)
        image_path = Path(image_path)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and image_path_str in self._image_cache:
            return self._image_cache[image_path_str]
        
        if not image_path.exists():
            raise FileNotFoundError(f"Image file not found: {image_path}")
        
        # å‹ç¼©å›¾ç‰‡
        image_data = self._compress_image(image_path)
        
        # ç¼–ç ä¸º base64
        encoded = base64.b64encode(image_data).decode('utf-8')
        
        # æ›´æ–°ç¼“å­˜ï¼ˆLRU ç­–ç•¥ï¼‰
        if use_cache:
            if len(self._image_cache) >= self._cache_max_size:
                # åˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•ç­–ç•¥ï¼šåˆ é™¤ç¬¬ä¸€ä¸ªï¼‰
                oldest_key = next(iter(self._image_cache))
                del self._image_cache[oldest_key]
            self._image_cache[image_path_str] = encoded
        
        return encoded
    
    def classify_flood_relevance(
        self,
        title: str,
        transcription: str,
        hashtags: str,
        image_paths: List[str],
        project_root: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ VLM åˆ¤æ–­å¸–å­æ˜¯å¦ä¸æ´ªæ°´ç›¸å…³
        
        Returns:
            Dict with keys: is_flood_related (bool), confidence (str), reason (str)
        """
        # æ„å»ºæ–‡æœ¬ä¿¡æ¯ï¼ŒæŒ‰ä¼˜å…ˆçº§ç»„ç»‡
        # ä¼˜å…ˆçº§ï¼šTitle > Hashtags > Transcription (transcriptionå¯èƒ½ä¸å‡†ç¡®)
        reliable_text_parts = []
        if title and str(title).strip() and str(title).strip().lower() != 'nan':
            reliable_text_parts.append(f"Title: {title}")
        if hashtags and str(hashtags).strip() and str(hashtags).strip().lower() != 'nan':
            reliable_text_parts.append(f"Hashtags: {hashtags}")
        
        reliable_text = "\n".join(reliable_text_parts) if reliable_text_parts else None
        
        # Transcription å•ç‹¬å¤„ç†ï¼Œæ ‡è®°ä¸ºå¯èƒ½ä¸å¯é 
        has_transcription = False
        transcription_text = None
        if transcription and str(transcription).strip() and str(transcription).strip().lower() != 'nan':
            has_transcription = True
            transcription_text = str(transcription).strip()
        
        # é™åˆ¶å›¾ç‰‡æ•°é‡ï¼ˆæœ€å¤šä½¿ç”¨3å¼ å…³é”®å¸§ï¼Œä»¥å‡å°‘ IO å¹¶å‘ï¼‰
        total_frames = len(image_paths)
        max_images = 3
        image_paths_limited = image_paths[:max_images] if total_frames > max_images else image_paths
        
        # åŠ è½½å›¾åƒ
        images_base64 = []
        for img_path in image_paths_limited:
            try:
                if project_root:
                    full_path = project_root / img_path
                else:
                    full_path = Path(img_path)
                
                if full_path.exists():
                    img_b64 = self.encode_image(str(full_path))
                    images_base64.append(img_b64)
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to load image {img_path}: {e}")
                continue
        
        if not images_base64:
            print("âš ï¸  Warning: No images available for this post")
            return {
                "is_flood_related": False,
                "confidence": "low",
                "reason": "No images available"
            }
        
        # æ„å»º promptï¼ˆç²¾ç®€ç‰ˆï¼‰
        prompt_parts = [
            "Analyze if this TikTok post is flood-related. Consider ALL sources (title, hashtags, transcription, images) and make a COMPREHENSIVE judgment.",
            "",
            "Return TRUE if floods/flooding is the PRIMARY content:",
            "- Visual: flooded areas, water damage, rescue operations",
            "- Text: discussions of floods, impacts, events",
            "- News/reporting about flooding",
            "- Political/social commentary where flooding is MAIN topic",
            "",
            "Return FALSE if:",
            "- Only passing mention (not main topic)",
            "- Unrelated content using water words",
            "- Visual contradicts text claims",
            "",
            "Analysis: (1) Examine each source, (2) Check consistency, (3) Determine if flooding is PRIMARY subject",
            "",
            "Strong indicators (TRUE): Multiple sources mention floods consistently, flood hashtags (#flood, #bangladeshfloods), detailed flood discussion, visual flood evidence.",
            "",
            "Weak indicators (consider context): Brief mention, generic water imagery, metaphorical flood terms.",
            ""
        ]
        
        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯
        if reliable_text:
            prompt_parts.append(f"Title/Hashtags: {reliable_text}")
            prompt_parts.append("")
        
        # æ·»åŠ transcription
        if has_transcription and transcription_text:
            prompt_parts.append(f"Audio/Speech: {transcription_text}")
            prompt_parts.append("")
        
        if total_frames > len(images_base64):
            visual_info = f"Visual: {len(images_base64)} key frame(s) attached (selected from {total_frames} total frames)."
        else:
            visual_info = f"Visual: {len(images_base64)} key frame(s) attached."
        
        prompt_parts.extend([
            visual_info,
            "",
            "Respond ONLY with JSON:",
            '{"is_flood_related": true/false, "confidence": "high/medium/low", "reason": "brief explanation"}'
        ])
        
        prompt = "\n".join(prompt_parts)
        
        # æ„å»ºæ¶ˆæ¯
        message = {
            "role": "user",
            "content": prompt,
            "images": images_base64
        }
        
        # å‘é€è¯·æ±‚
        payload = {
            "model": self.model,
            "messages": [message],
            "stream": False
        }
        
        # åŒæ­¥è¯·æ±‚ä¹Ÿæ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        last_error = None
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{self.base_url}/api/chat",
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                )
                response.raise_for_status()
                result = response.json()
                
                # æå–å“åº”å†…å®¹
                content = result.get("message", {}).get("content", "")
                
                # è§£æ JSON å“åº”
                return self._parse_response(content)
                
            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, BrokenPipeError, OSError) as e:
                last_error = e
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    print(f"âš ï¸  Connection error (attempt {attempt + 1}/{max_retries}): {type(e).__name__}. Retrying in {wait_time}s...")
                    import time
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âŒ Max retries reached. Error: {type(e).__name__}")
                    return {
                        "is_flood_related": False,
                        "confidence": "low",
                        "reason": "API connection failed after retries"
                    }
            except Exception as e:
                print(f"âŒ Unexpected error: {type(e).__name__}: {str(e)[:100]}")
                return {
                    "is_flood_related": False,
                    "confidence": "low",
                    "reason": "Processing error"
                }
        
        return {
            "is_flood_related": False,
            "confidence": "low",
            "reason": "API request failed"
        }
    
    def _parse_response(self, content: str) -> Dict[str, Any]:
        """è§£ææ¨¡å‹å“åº”ï¼Œæå– JSON"""
        # å°è¯•æå– JSON å¯¹è±¡
        # æ–¹æ³•1: ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡
        json_match = re.search(r'\{[^{}]*"is_flood_related"[^{}]*\}', content, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                return {
                    "is_flood_related": bool(parsed.get("is_flood_related", False)),
                    "confidence": str(parsed.get("confidence", "low")).lower(),
                    "reason": str(parsed.get("reason", ""))
                }
            except json.JSONDecodeError:
                pass
        
        # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ JSON
        json_patterns = [
            r'\{[^}]*"is_flood_related"[^}]*\}',
            r'is_flood_related["\s]*:\s*(true|false)',
            r'"is_flood_related"["\s]*:\s*(true|false)',
        ]
        
        for pattern in json_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                if 'true' in match.group(0).lower():
                    return {
                        "is_flood_related": True,
                        "confidence": "medium",
                        "reason": "Parsed from response"
                    }
                elif 'false' in match.group(0).lower():
                    return {
                        "is_flood_related": False,
                        "confidence": "medium",
                        "reason": "Parsed from response"
                    }
        
        # æ–¹æ³•3: åŸºäºå…³é”®è¯çš„å¯å‘å¼åˆ¤æ–­
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in ["related", "flood", "yes", "true"]):
            if any(keyword in content_lower for keyword in ["not", "unrelated", "false", "no"]):
                is_related = False
            else:
                is_related = True
        else:
            is_related = False
        
        return {
            "is_flood_related": is_related,
            "confidence": "low",
            "reason": f"Fallback parsing: {content[:200]}"
        }
    
    def _build_prompt(
        self,
        title: str,
        transcription: str,
        hashtags: str,
        image_paths: List[str],
        project_root: Optional[Path] = None,
        total_frames: Optional[int] = None
    ) -> Tuple[str, List[str]]:
        """
        æ„å»º prompt å’ŒåŠ è½½å›¾åƒï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¾›åŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬å…±ç”¨ï¼‰
        
        Returns:
            (prompt, images_base64)
        """
        # æ„å»ºæ–‡æœ¬ä¿¡æ¯
        reliable_text_parts = []
        if title and str(title).strip() and str(title).strip().lower() != 'nan':
            reliable_text_parts.append(f"Title: {title}")
        if hashtags and str(hashtags).strip() and str(hashtags).strip().lower() != 'nan':
            reliable_text_parts.append(f"Hashtags: {hashtags}")
        
        reliable_text = "\n".join(reliable_text_parts) if reliable_text_parts else None
        
        has_transcription = False
        transcription_text = None
        if transcription and str(transcription).strip() and str(transcription).strip().lower() != 'nan':
            has_transcription = True
            transcription_text = str(transcription).strip()
        
        # åŠ è½½å›¾åƒï¼ˆä½¿ç”¨å…¨éƒ¨å…³é”®å¸§ï¼‰
        images_base64 = []
        for img_path in image_paths:
            try:
                if project_root:
                    full_path = project_root / img_path
                else:
                    full_path = Path(img_path)
                
                if full_path.exists():
                    img_b64 = self.encode_image(str(full_path))
                    images_base64.append(img_b64)
            except Exception as e:
                continue
        
        if not images_base64:
            raise ValueError("No images available")
        
        # æ„å»º promptï¼ˆç²¾ç®€ç‰ˆï¼‰
        prompt_parts = [
            "Analyze if this TikTok post is flood-related. Consider ALL sources (title, hashtags, transcription, images) and make a COMPREHENSIVE judgment.",
            "",
            "Return TRUE if floods/flooding is the PRIMARY content:",
            "- Visual: flooded areas, water damage, rescue operations",
            "- Text: discussions of floods, impacts, events",
            "- News/reporting about flooding",
            "- Political/social commentary where flooding is MAIN topic",
            "",
            "Return FALSE if:",
            "- Only passing mention (not main topic)",
            "- Unrelated content using water words",
            "- Visual contradicts text claims",
            "",
            "Analysis: (1) Examine each source, (2) Check consistency, (3) Determine if flooding is PRIMARY subject",
            "",
            "Strong indicators (TRUE): Multiple sources mention floods consistently, flood hashtags (#flood, #bangladeshfloods), detailed flood discussion, visual flood evidence.",
            "",
            "Weak indicators (consider context): Brief mention, generic water imagery, metaphorical flood terms.",
            ""
        ]
        
        if reliable_text:
            prompt_parts.append(f"Title/Hashtags: {reliable_text}")
            prompt_parts.append("")
        
        if has_transcription and transcription_text:
            prompt_parts.append(f"Audio/Speech: {transcription_text}")
            prompt_parts.append("")
        
        # æ˜¾ç¤ºå›¾ç‰‡æ•°é‡ä¿¡æ¯
        if total_frames and total_frames > len(images_base64):
            visual_info = f"Visual: {len(images_base64)} key frame(s) attached (selected from {total_frames} total frames)."
        else:
            visual_info = f"Visual: {len(images_base64)} key frame(s) attached."
        
        prompt_parts.extend([
            visual_info,
            "",
            "Respond ONLY with JSON:",
            '{"is_flood_related": true/false, "confidence": "high/medium/low", "reason": "brief explanation"}'
        ])
        
        prompt = "\n".join(prompt_parts)
        return prompt, images_base64
    
    async def _send_classification_request(
        self,
        prompt: str,
        images_base64: List[str],
        session: Optional[aiohttp.ClientSession] = None,
        max_retries: int = 5
    ) -> Dict[str, Any]:
        """
        å‘é€åˆ†ç±»è¯·æ±‚ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯
        message = {
            "role": "user",
            "content": prompt,
            "images": images_base64
        }
        
        # å‘é€å¼‚æ­¥è¯·æ±‚ï¼ˆå¸¦ keep_alive å’Œä¼˜åŒ–é€‰é¡¹ï¼‰
        payload = {
            "model": self.model,
            "messages": [message],
            "stream": False,
            "options": {
                "num_gpu": 999,  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
                "num_ctx": 1536,  # ä¸Šä¸‹æ–‡çª—å£
                "num_batch": 1024,  # æ‰¹å¤„ç†å¤§å°
                "kv_cache_type": "q8_0",  # KVç¼“å­˜ç±»å‹
                "use_mmap": True  # æœ¬æœºNVMeä½¿ç”¨mmapï¼Œç½‘ç»œç›˜è®¾ä¸ºFalse
            },
            "keep_alive": "2h"  # ä¿æŒæ¨¡å‹åŠ è½½2å°æ—¶
        }
        
        last_error = None
        for attempt in range(max_retries):
            try:
                if session is None:
                    async with aiohttp.ClientSession() as temp_session:
                        async with temp_session.post(
                            f"{self.base_url}/api/chat",
                            json=payload,
                            headers={"Content-Type": "application/json"},
                            timeout=aiohttp.ClientTimeout(total=600, connect=30)
                        ) as response:
                            response.raise_for_status()
                            result = await response.json()
                            content = result.get("message", {}).get("content", "")
                            return self._parse_response(content)
                else:
                    async with session.post(
                        f"{self.base_url}/api/chat",
                        json=payload,
                        headers={"Content-Type": "application/json"},
                        timeout=aiohttp.ClientTimeout(total=600, connect=30)
                    ) as response:
                        response.raise_for_status()
                        result = await response.json()
                        content = result.get("message", {}).get("content", "")
                        return self._parse_response(content)
                        
            except (aiohttp.ClientError, ConnectionError, BrokenPipeError, OSError, aiohttp.ClientConnectorError, aiohttp.ServerDisconnectedError) as e:
                last_error = e
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3  # 3s, 6s, 9s, 12s, 15s
                    await asyncio.sleep(wait_time)
                    # ä¸æ‰“å°é‡è¯•ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    continue
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
                    raise Exception(f"API connection failed after {max_retries} retries: {type(e).__name__}")
            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise Exception(f"Processing error after {max_retries} retries: {type(e).__name__}: {str(e)[:100]}")
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
        raise Exception("API request failed after all retries")


async def process_csv_async(
    csv_path: str,
    output_csv_path: Optional[str] = None,
    model: str = "qwen3-vl:30b-a3b-instruct-q4_K_M",
    base_url: str = "http://127.0.0.1:11434",
    start_idx: int = 0,
    max_rows: Optional[int] = None,
    resume: bool = True,
    max_concurrent: int = 2
):
    """
    å¼‚æ­¥å¤„ç† CSV æ–‡ä»¶ï¼Œæ·»åŠ æ´ªæ°´ç›¸å…³æ€§æ ‡æ³¨ï¼ˆæ”¯æŒå¹¶å‘æ‰¹é‡å¤„ç†ï¼‰
    
    Args:
        csv_path: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„
        output_csv_path: è¾“å‡º CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        model: Ollama æ¨¡å‹åç§°
        base_url: Ollama API åŸºç¡€ URL
        start_idx: å¼€å§‹å¤„ç†çš„ç´¢å¼•ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        max_rows: æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆNone è¡¨ç¤ºå¤„ç†æ‰€æœ‰ï¼‰
        resume: æ˜¯å¦è·³è¿‡å·²æœ‰æ ‡æ³¨çš„è¡Œ
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤2ï¼Œé¿å…è¿æ¥é”™è¯¯ï¼‰
    """
    print("ğŸŒŠ Starting flood relevance classification (Async Mode)")
    print(f"âš¡ Max concurrent requests: {max_concurrent}")
    print("=" * 70)
    
    # è¯»å– CSV
    print(f"\nğŸ“– Reading CSV: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"âœ… Loaded {len(df)} rows")
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if output_csv_path is None:
        output_csv_path = csv_path
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    csv_path_obj = Path(csv_path).resolve()
    if csv_path_obj.parts[-3] == "csvs":
        project_root = csv_path_obj.parent.parent.parent.parent
    else:
        project_root = Path(csv_path).resolve()
        while project_root.parent != project_root:
            if any(p in ["tiktok", "twitter"] for p in project_root.parts):
                if project_root.name in ["tiktok", "twitter"]:
                    project_root = project_root.parent
                break
            project_root = project_root.parent
        if project_root == Path(csv_path).resolve():
            project_root = Path.cwd()
    
    # æ·»åŠ æ–°åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    classification_column = "is_flood_related"
    confidence_column = "flood_classification_confidence"
    reason_column = "flood_classification_reason"
    
    if classification_column not in df.columns:
        df[classification_column] = None
    if confidence_column not in df.columns:
        df[confidence_column] = None
    if reason_column not in df.columns:
        df[reason_column] = None
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = OllamaVLMClassifier(base_url=base_url, model=model)
    
    # åˆ›å»ºå…¨å±€ aiohttp.ClientSessionï¼ˆå¤ç”¨è¿æ¥ï¼‰
    connector = aiohttp.TCPConnector(limit=max_concurrent * 2, limit_per_host=max_concurrent)
    timeout = aiohttp.ClientTimeout(total=600, connect=30)
    global_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
    
    try:
        # ç¡®å®šå¤„ç†èŒƒå›´
        end_idx = len(df) if max_rows is None else min(start_idx + max_rows, len(df))
        rows_to_process = df.iloc[start_idx:end_idx]
        
        print(f"\nğŸ“Š Processing rows {start_idx} to {end_idx-1} (total: {len(rows_to_process)} rows)")
        print("-" * 70)
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        task_indices = []
        
        for idx, row in rows_to_process.iterrows():
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            if resume and pd.notna(row.get(classification_column)):
                reason_val = row.get(reason_column)
                # å¦‚æœä¹‹å‰è®°å½•çš„æ˜¯é”™è¯¯ä¿¡æ¯ï¼Œåˆ™è§†ä¸ºæœªå¤„ç†ï¼Œéœ€é‡æ–°å°è¯•
                if isinstance(reason_val, str) and "error" in reason_val.lower():
                    df.at[idx, classification_column] = None
                    df.at[idx, confidence_column] = None
                    df.at[idx, reason_column] = None
                else:
                    continue
            
            # æå–æ•°æ®
            title = row.get("title", "")
            transcription = row.get("transcription_english", "")
            hashtags = row.get("hashtags", "")
            
            # è§£æ key_frames
            key_frames_str = row.get("key_frames", "")
            image_paths = []
            
            if pd.notna(key_frames_str) and str(key_frames_str).strip():
                try:
                    if isinstance(key_frames_str, str):
                        image_paths = json.loads(key_frames_str)
                    elif isinstance(key_frames_str, list):
                        image_paths = key_frames_str
                except Exception:
                    pass
            
            if not image_paths:
                # æ ‡è®°ä¸ºæ— å›¾åƒ
                df.at[idx, classification_column] = False
                df.at[idx, confidence_column] = "low"
                df.at[idx, reason_column] = "No key frames available"
                continue
            
            # é™åˆ¶å›¾ç‰‡æ•°é‡ï¼šåªä½¿ç”¨å‰1-2å¼ å…³é”®å¸§ï¼ˆE. é™åˆ¶å›¾åƒå¹¶å‘ï¼‰
            max_images = 3
            image_paths_limited = image_paths[:max_images]
            
            # åˆ›å»ºä»»åŠ¡
            tasks.append((idx, title, transcription, hashtags, image_paths_limited, image_paths))
            task_indices.append(idx)
    
        if not tasks:
            print("âœ… All rows already processed or no tasks to process")
            return
        
        print(f"ğŸš€ Processing {len(tasks)} tasks with {max_concurrent} concurrent requests...")
        print("=" * 70)
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        processed_count = asyncio.Lock()
        processed_num = [0]
        error_count = [0]
        session_lock = asyncio.Lock()
        
        async def process_single_task(task_data):
            """å¤„ç†å•ä¸ªä»»åŠ¡"""
            nonlocal global_session
            idx, title, transcription, hashtags, image_paths_limited, image_paths_all = task_data
            row_num = idx + 1
            
            async with semaphore:
                try:
                    # æ„å»º promptï¼ˆä½¿ç”¨é™åˆ¶åçš„å›¾ç‰‡ï¼‰
                    prompt, images_base64 = classifier._build_prompt(
                        title=title,
                        transcription=transcription,
                        hashtags=hashtags,
                        image_paths=image_paths_limited,  # åªä½¿ç”¨1-3å¼ 
                        project_root=project_root,
                        total_frames=len(image_paths_all)  # ä¼ é€’æ€»æ•°ç”¨äºæç¤º
                    )
                    
                    # ä½¿ç”¨å…¨å±€ sessionï¼ˆå¤ç”¨è¿æ¥ï¼‰ï¼Œå¦‚é‡è¿æ¥é”™è¯¯å°è¯•é‡å»º session ä¸€æ¬¡
                    attempt_result = None
                    for session_attempt in range(2):
                        try:
                            attempt_result = await classifier._send_classification_request(
                                prompt=prompt,
                                images_base64=images_base64,
                                session=global_session
                            )
                            break
                        except Exception as request_error:
                            error_message = str(request_error)
                            if session_attempt == 0 and "API connection failed" in error_message:
                                # é‡æ–°åˆ›å»º session
                                async with session_lock:
                                    await global_session.close()
                                    connector = aiohttp.TCPConnector(limit=max_concurrent * 2, limit_per_host=max_concurrent)
                                    timeout = aiohttp.ClientTimeout(total=600, connect=30)
                                    global_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
                                await asyncio.sleep(1)
                                continue
                            raise
                    if attempt_result is None:
                        raise Exception("Failed to obtain classification result")
                    result = attempt_result
                    
                    # æ›´æ–° DataFrame
                    df.at[idx, classification_column] = result["is_flood_related"]
                    df.at[idx, confidence_column] = result["confidence"]
                    df.at[idx, reason_column] = result["reason"]
                    
                    # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°è®¡æ•°
                    async with processed_count:
                        processed_num[0] += 1
                        current_count = processed_num[0]
                    
                    # åªæ‰“å°ç»“æœå’Œè¿›åº¦
                    status = "âœ… RELATED" if result["is_flood_related"] else "âŒ NOT RELATED"
                    print(f"[{current_count}/{len(tasks)}] Row {row_num}: {status} (confidence: {result['confidence'].upper()})")
                    
                    # æ¯å¤„ç†5ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡
                    if current_count % 5 == 0:
                        df.to_csv(output_csv_path, index=False)
                        print(f"ğŸ’¾ Progress saved: {current_count}/{len(tasks)} processed")
                    
                except Exception as e:
                    async with processed_count:
                        error_count[0] += 1
                        processed_num[0] += 1
                        current_count = processed_num[0]
                    
                    # ä¸å†™å…¥é”™è¯¯ä¿¡æ¯åˆ° CSVï¼Œä¿ç•™ä¸ºç©ºï¼Œè®©ç”¨æˆ·å¯ä»¥ç¨åé‡è¯•
                    # è¿™æ · resume æ¨¡å¼ä¼šè‡ªåŠ¨é‡è¯•è¿™äº›å¤±è´¥çš„è¡Œ
                    error_msg = str(e)[:100]
                    print(f"âŒ [{current_count}/{len(tasks)}] Row {row_num}: Failed after retries - {error_msg}")
                    # ä¿ç•™ä¸º Noneï¼Œä¸å†™å…¥ CSVï¼Œè¿™æ · resume æ—¶ä¼šè‡ªåŠ¨é‡è¯•
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*[process_single_task(task) for task in tasks])
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\nğŸ’¾ Saving final results to: {output_csv_path}")
        df.to_csv(output_csv_path, index=False)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 70)
        print("ğŸ“Š Summary:")
        print(f"   Total rows: {len(df)}")
        successful_count = processed_num[0] - error_count[0]
        print(f"   âœ… Successfully processed: {successful_count}")
        print(f"   â­ï¸  Skipped (already processed): {len(rows_to_process) - len(tasks)}")
        print(f"   âŒ Failed (will retry on resume): {error_count[0]}")
        
        if classification_column in df.columns:
            related_count = df[classification_column].sum() if df[classification_column].dtype == bool else df[classification_column].eq(True).sum()
            print(f"   ğŸŒŠ Flood-related posts: {related_count}")
        
        if error_count[0] > 0:
            print(f"\n   ğŸ’¡ Tip: Re-run the script to retry {error_count[0]} failed rows")
        
        print(f"   Output saved to: {output_csv_path}")
        print("=" * 70)
    
    finally:
        # å…³é—­å…¨å±€ session
        await global_session.close()


def process_csv(
    csv_path: str,
    output_csv_path: Optional[str] = None,
    model: str = "qwen3-vl:30b-a3b-instruct-q4_K_M",
    base_url: str = "http://127.0.0.1:11434",
    start_idx: int = 0,
    max_rows: Optional[int] = None,
    resume: bool = True,
    max_concurrent: int = 3
):
    """
    åŒæ­¥åŒ…è£…å‡½æ•°ï¼Œè°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬
    """
    asyncio.run(process_csv_async(
        csv_path=csv_path,
        output_csv_path=output_csv_path,
        model=model,
        base_url=base_url,
        start_idx=start_idx,
        max_rows=max_rows,
        resume=resume,
        max_concurrent=max_concurrent
    ))


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Ollama VLM å¯¹ TikTok æ•°æ®è¿›è¡Œæ´ªæ°´ç›¸å…³æ€§æ ‡æ³¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¤„ç†æ•´ä¸ª CSV æ–‡ä»¶
  python classify_flood_relevance.py tiktok/assam_flood/csvs/filtered_assam_flood_posts_20240501_20241120_with_local_paths.csv
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python classify_flood_relevance.py input.csv -o output.csv
  
  # ä»ç¬¬10è¡Œå¼€å§‹å¤„ç†ï¼Œæœ€å¤šå¤„ç†20è¡Œ
  python classify_flood_relevance.py input.csv --start-idx 10 --max-rows 20
  
  # ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°å¤„ç†æ‰€æœ‰è¡Œï¼‰
  python classify_flood_relevance.py input.csv --no-resume
  
  # è‡ªå®šä¹‰å¹¶å‘æ•°ï¼ˆåŠ é€Ÿå¤„ç†ï¼‰
  python classify_flood_relevance.py input.csv --max-concurrent 5
        """
    )
    
    parser.add_argument(
        "csv_path",
        help="CSV æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "-o", "--output",
        help="è¾“å‡º CSV æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–åŸæ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--model",
        default="qwen3-vl:30b-a3b-instruct-q4_K_M",
        help="Ollama æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--base-url",
        default="http://127.0.0.1:11434",
        help="Ollama API åŸºç¡€ URL"
    )
    parser.add_argument(
        "--start-idx",
        type=int,
        default=0,
        help="å¼€å§‹å¤„ç†çš„ç´¢å¼•ï¼ˆé»˜è®¤: 0ï¼‰"
    )
    parser.add_argument(
        "--max-rows",
        type=int,
        help="æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆé»˜è®¤: å¤„ç†æ‰€æœ‰è¡Œï¼‰"
    )
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°å¤„ç†æ‰€æœ‰è¡Œï¼‰"
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=3,
        help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆé»˜è®¤: 3ï¼Œå¯æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´ï¼‰"
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(args.csv_path):
        print(f"âŒ CSV file not found: {args.csv_path}")
        return
    
    process_csv(
        csv_path=args.csv_path,
        output_csv_path=args.output,
        model=args.model,
        base_url=args.base_url,
        start_idx=args.start_idx,
        max_rows=args.max_rows,
        resume=not args.no_resume,
        max_concurrent=args.max_concurrent
    )
    
    print("\nğŸ‰ Classification completed!")


if __name__ == "__main__":
    main()

